%% Master Thesis Table of Contents
%% Integrating Object-Centric Learning with Model Based Reinforcement Learning

\documentclass[
	english,
	ruledheaders=section,
	class=report,
	thesis={type=master},
	accentcolor=9c,
	custommargins=true,
	marginpar=false,
	parskip=half-,
	fontsize=11pt,
]{tudapub}



\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}
\fi



%%%%%%%%%%%%%%%%%%%
% Language settings
%%%%%%%%%%%%%%%%%%%
\usepackage[english]{babel}
\selectlanguage{english}
\usepackage[autostyle]{csquotes}
\usepackage{microtype}


%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%
\usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\addbibresource{references.bib}


\DefineBibliographyStrings{english}{
  bibliography = {References},
}





%%%%%%%%%%%%%%%%%%%
% Table packages
%%%%%%%%%%%%%%%%%%%
\usepackage{tabularx}
\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%
% Math packages
%%%%%%%%%%%%%%%%%%%
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%
% Additional packages
%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%
% Custom macros
%%%%%%%%%%%%%%%%%%%
\newcommand{\worldmodel}{world model}

\begin{document}

\Metadata{
	title=Structured World Understanding: Integrating Object-Centric Learning with Model Based Reinforcement Learning,
	author=Your Name
}

\title{Structured World Understanding: Integrating Object-Centric Learning with Model Based Reinforcement Learning}
\subtitle{Master's Thesis: Summer Semester 2025}
\author[Y. Name]{Your Full Name}
\studentID{1234567}
\reviewer{Prof. Dr. Kristian Kersting\\[0.2em]2. Review: Jannis Blüml, Raban Emunds}

\department{Department of Computer Science}
\group{Machine Learning Research Group (AI \& ML Lab)}

\submissiondate{24.10.2025}
\examdate{24.10.2025}

\maketitle

\affidavit

\tableofcontents
\listoffigures
\listoftables

% List of Abbreviations
\chapter*{List of Abbreviations}
\addcontentsline{toc}{chapter}{List of Abbreviations}
\begin{tabular}{ll}
	RL   & Reinforcement Learning              \\
	MBRL & Model Based Reinforcement Learning  \\
	MFRL & Model Free Reinforcement Learning   \\
	CNN  & Convolutional Neural Network        \\
	LSTM & Long Short-Term Memory              \\
	PPO  & Proximal Policy Optimization        \\
	A3C  & Asynchronous Advantage Actor-Critic \\
	DQN  & Deep Q-Network                      \\
	RSSM & Recurrent State Space Model         \\
	KL   & Kullback-Leibler                    \\
	MSE  & Mean Squared Error                  \\
	RGB  & Red-Green-Blue                      \\
	GPU  & Graphics Processing Unit            \\
	JAX  & Just After eXecution                \\
\end{tabular}

\chapter{Introduction}
\label{chap:introduction}

Modern Artificial Intelligence approaches often struggle with understanding and
reasoning about complex environments in a structured, interpretable way.
Reinforcement Learning in particular suffers from agents learning control tasks
or games especially well without grasping the underlying rules pf the game.
Unlike Supervised Learning for example there is no ground truth from the which
the agent can learn from. Among numerous distinctions within reinforcement
rearning there is the difference between Model Based Reinforcement Learning and
Model Free Reinforcement Learning. The latter focuses on the learning of only
the agent which is deployed in the environment (or learns from samples). Thus
the agent will have no explicit way of predicting the next state of the
environment. Model based Reinforcement learning augments the architecture by a
model that is predicting the next state of the world but also still trains an
agent. Although model free approaches usually are less complex since they only
have to learn one model (the agent), they also have significant disadvantages.

Model free learning often struggles with sample efficiency and generalization
to new tasks. Small changes in the environment can lead to significant drops in
performance. Moreover, agents can learn misaligned policies that exploit
loopholes in the environment rather than achieving the intended goals making
explainability of such agents quite hard. Moreover Reinforcement Learning and
model free approaches in particular are notoriously data hungr often requiring
millions of samples to learn reasonable policies. For Real World applications
or robotics this is often not feasible as sampling might involve wearing down
expensive machinery.

A solution to the lack of structure and interpretability can be Object
Centricity, which focuses on learning or providing some kind of object
representation that can be used for reasoning and decision-making. This can
involve identifying and manipulating objects within the environment, leading to
more interpretable and robust policies. Built on top of Atari OCAtari
\cite{delfosse2023ocatari} provides a standardized framework for extracting
object representations from the standard Atari environment. This makes
different object centric appraoches more comparable.

Object Centricity has the goal of interpretability and Robustness whereas Model
Based approaches try to improve sample efficiency and planning capabilities
among others. This thesis aims to investigate the potential benefits of
integrating object-centric learning into model based reinforcement learning
frameworks to address these challenges by harvesting the strengths of both
paradigms. This thesis is therefore guided by the following research questions:
\begin{enumerate}
	\item How do object-centric \worldmodel{}s compare to pixel-based \worldmodel{}s
	      regarding prediction accuracy and sample efficiency?
	\item Can an actor-critic framework be effectively trained on simulated rollouts from
	      object-centric \worldmodel{}s?
	\item What is the impact of different \worldmodel{} architectures (such as LSTM-based
	      versus alternative designs) on the overall performance of object-centric model
	      based reinforcement learning in structured environments?
	\item Does the object-centric inductive bias in \worldmodel{}s improve robustness to
	      changes in the environment?
\end{enumerate}

Within this thesis we propose to develop a \worldmodel{} that is running on top
of object centric states provided by the a framework called JAXAtari
\cite{jaxatari}. This \worldmodel{} is trained to predict the next state of the
environment given the current state and action. An actor-critic framework based
on the DreamerV2 \cite{hafner2020dream} architecture is integrated with the
\worldmodel{} to learn policies from imagined rollouts. The training pipeline
involves alternating between updating the \worldmodel{} and optimizing the
policy based on simulated experiences. The performance of the proposed approach
will be evaluated in the Pong environment from the Atari benchmark suite,
comparing it against pixel-based baselines and assessing its sample efficiency
and robustness. Combining both appraoches thus offers the possibility of
complex planning using object centric states. Although out of the scope of this
thesis one could imagine interpretable planned stategies describing how certain
objects influence decisions of the agent. This is usually quite hard with
existing model based approaches since they usually rely on latent space which
is then used to make predictions resulting in the loss of human
interpretability.

The remainder of this thesis is structured as follows: Chapter 2 provides
background information on reinforcement learning, model based approaches, and
object-centric representations. Chapter 3 details the methodology, including
the \worldmodel{} architecture, actor-critic integration, and training
pipeline. Chapter 4 presents the experimental setup, results, and analysis.
Chapter 5 discusses the findings, challenges, and implications of the results.
Finally, Chapter 6 concludes the thesis and outlines directions for future
work.

\chapter{Background and Related Work}
\label{chap:background}
This chapter aims to give an overview into the relevant mathematical background and related work
in the fields of Reinforcement Learning, Model Based Reinforcement Learning and Object-Centric Learning.
\section{Reinforcement Learning}
\label{sec:reinforcement_learning}

Reinforcement Learning (RL) is a machine learning subcategory that focuses on
training agents that make decision in an environment. An environment can be a
game, a robotic control task or any other sequential decision making problem.
To give that a more structured formulation the definition of a Markov Decision
Process (MDP) is used. This term comes from the field of operations research
and was named after the Russian mathematician Andrey Markov in relation to his
work on stochastic processes. An MDP is defined as a tuple $\mathcal{M} =
	(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, where:

\begin{itemize}
	\item $\mathcal{S}$ is the state space
	\item $\mathcal{A}$ is the action space
	\item $P(s'|s,a)$ is the transition probability function defining the probability of transitioning to state $s'$ given current state $s$ and action $a$
	\item $R(s,a,s')$ is the reward function specifying the immediate reward received after taking action $a$ in state $s$ and transitioning to state $s'$
	\item $\gamma \in [0,1]$ is the discount factor that determines the importance of future rewards. This is also often used to prove convergence of certain algorithms.
\end{itemize}

Markov Chains assume the Markov property, which states that the future state
depends only on the current state and action, not on the entire history of past
states and actions. This memoryless property is what enabled many
computationally viable algorithms to be developed in the first place since
every probability computed is only conditioned on the current state and action
so a finite set of values instead of a potentially infinite set of past states
and actions.

Given that we now defined what the environment actually means let us examine
the goal of the agent. The agent's goal is to learn an optimal policy
$\pi^*(s)$ that maximizes the expected cumulative discounted reward, known as
the return \cite{sutton2018reinforcement}:
\begin{equation}
	G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}

The value function $V^\pi(s)$ represents the expected return when starting from
state $s$ and following policy $\pi$:
\begin{equation}
	V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]
\end{equation}

In a similar manner, $Q^\pi(s,a)$ represents the expected return when starting
from state $s$, taking action $a$, and then following policy $\pi$. Making the
action value function a function of both state and action. This can sometimes
be more useful since it directly reveals the value of taking a certain action
as well.

Model free reinforcement learning approaches learn optimal policies directly
from experience without explicitly modeling the environment dynamics. These
methods estimate value functions or policies through trial-and-error
interactions with the environment, making them broadly applicable but
potentially sample-inefficient.

As mentioned in the introduction, model free rely on learning an agent directly
by interacting with the environment and receiving feedback in the form of
rewards. This trial-and-error process allows the agent to improve its policy
over time. By omitting a \worldmodel{} the complexity of the overall system is
reduced but the the agent is usually quite sample inefficient
\cite{duan2016benchmarking,kaiser2019model}. Also, the lack of a model can make
it difficult for the agent to plan ahead or consider the long-term consequences
of its actions. On top of that \worldmodel{}s can improve generalization to new
tasks and make the agent more robust \cite{ha2018world,hafner2020dream}.
Moreover they provide limited interpretability with regards to their learned
policy.

Popular model free algorithms include temporal difference methods like
Q-Learning \cite{watkins1989learning} and SARSA \cite{rummery1994line}, as well
as policy gradient methods such as REINFORCE \cite{williams1992simple},
Actor-Critic \cite{barto1983neuronlike}, and Proximal Policy Optimization (PPO)
\cite{schulman2017proximal}.

Model based reinforcement learning addresses some limitations of model free
approaches by learning an explicit model of the environment dynamics.

In MBRL the dynamics predictor learns to predict next states and rewards given
current states and actions:

\begin{align}
	\hat{s}_{t+1} & = f_\theta(s_t, a_t)        \\
	\hat{r}_{t+1} & = g_\phi(s_t, a_t, s_{t+1})
\end{align}

where $f_\theta$ and $g_\phi$ represent parameterized functions (often neural
networks) that approximate the true environment dynamics.

Model based methods potentially have much better sample efficiency since it
takes less sampling in the environment to learn reasonable state transitions
than it takes to train a reinforcement learning. After the model has been
properly trained the agent can learn entirely within the \worldmodel{} making
additional sampling unnecessary. Although depending on game complexity the
initial sampling which is usually done by a random policy might not see diverse
enough states. This is for most Atari Games not an issue but for more complex
environment it can be important to retrain the \worldmodel{} with new
experience that is sampled by the newly trained agent in the real environment
\cite{janner2019trust,sutton1991dyna}. Moreover there are approaches that use
explicit planning strategies that hugely benefit interpretability. Using the
\worldmodel{} the agent can make guesses what might happen in the future and
explicitly reason why it decides for a certain action
\cite{schrittwieser2020mastering,greydanus2018visualizing}.

But training a well designed and stable model can create several challenges for
example the \worldmodel{} can introduce a bias into the agent which does not
hold in the true environment. The agent can learn to exploit errors in the
\worldmodel{} to artificially boost its score but then performs much worse
during inference \cite{talvitie2017self,lambert2020learning}.

\section{Dreamer Architecture}
\label{sec:dreamer_architecture}

The Dreamer architecture is a family of model based RL algorithms (DreamerV1,
V2, V3) that use a \worldmodel{} together with actor-critic methods. The main
idea is to learn a compact latent space where the agent can imagine rollouts
and train its policy without always interacting with the real environment.
Dreamer builds on ideas from PlaNet
\cite{hafner2019learninglatentdynamicsplanning} and models from
\cite{ha2018world}, which first used latent dynamics, but Dreamer adds
actor-critic learning and works better for Atari games.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{images/DreamerFlow.png}
	\caption{Overview of the DreamerV2 training pipeline. The \worldmodel{} learns to predict future states and rewards, enabling the actor-critic framework to optimize policies using imagined rollouts. Taken from Hafner et al.~\cite{hafner2021mastering}, Fig.~1.}
	\label{fig:dreamer_flow}
\end{figure}

DreamerV2 improves DreamerV1 by making it more stable and able to handle
discrete actions, which is important for Atari. DreamerV3 tries to make the
method work for many different tasks without changing hyperparameters. There
are different ways to train a \worldmodel{}. You can predict future
observations directly or predict latent states. In Dreamers case as shown in
Figure \ref{fig:dreamer_flow} the \worldmodel{} predicts latent states.

DreamerV2 builds upon the \worldmodel{} with an actor-critic framework. The
actor is responsible for selecting actions based on the current latent state,
while the critic evaluates the chosen actions by estimating their value.
Central to DreamerV2 is the use of the $\lambda$-return, which provides a
balance between bias and variance since using $\lambda=0$ leads to high bias
and low variance, while $\lambda=1$ leads to low bias and high variance.
Approaching TD(0) and Monte Carlo respectively. The following sections explain
in detail how the $\lambda$-target is computed and how it is used to define the
actor and critic.

The $\lambda$-target is defined recursively as:
\begin{equation}
	V^{\lambda}_t = \hat{r}_t + \hat{\gamma}_t \cdot \begin{cases}
		(1 - \lambda)v_\xi(\hat{z}_{t+1}) + \lambda V^{\lambda}_{t+1} & \text{if } t < H \\
		v_\xi(\hat{z}_H)                                              & \text{if } t = H
	\end{cases}
\end{equation}

Here, $\hat{r}_t$ is the predicted reward, $\hat{\gamma}_t$ is the predicted
discount, $v_\xi(\hat{z}_t)$ is the critic value, and $\lambda$ (usually 0.95)
controls how much you care about future rewards. This formula mixes short and
long-term returns.

The actor in DreamerV2 uses a loss that mixes different gradient estimators to
make learning stable. The actor tries to maximize the same $\lambda$-returns as
the critic, using both REINFORCE and backpropagation through the model.

The actor loss is:
\begin{align}
	\mathcal{L}(\psi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \Big[\right. & -\rho \ln p_\psi(\hat{a}_t | \hat{z}_t) \, \text{sg}(V^{\lambda}_t - v_\xi(\hat{z}_t)) \label{eq:reinforce} \\
	                                                                  & -(1-\rho)V^{\lambda}_t \label{eq:dynamics}                                                                  \\
	                                                                  & -\eta \mathcal{H}[a_t|\hat{z}_t] \Big] \label{eq:entropy}
\end{align}

The loss has three parts:

\textbf{REINFORCE} \eqref{eq:reinforce}: Maximizes the log-probability of actions, weighted by the advantage (difference between lambda-return and critic). The stop-gradient (sg) is used to not mess up the critic.

\textbf{Dynamics Backprop} \eqref{eq:dynamics}: Backpropagates through the model for more direct gradients.

\textbf{Entropy Regularization} \eqref{eq:entropy}: Encourages exploration by making the action distribution more random. $\eta$ controls how much exploration.

$\rho$ decides how much REINFORCE vs. backprop you use. For Atari, usually $\rho=1$ (just REINFORCE), $\eta=10^{-3}$.

The critic in DreamerV2 estimates the expected return from a latent state. It
is needed for both the lambda-targets and as a baseline for REINFORCE.

The critic is trained with TD learning using the lambda-targets:
\begin{equation}
	\mathcal{L}(\xi) = \mathbb{E}\left[\sum_{t=1}^{H-1} \frac{1}{2} \left(v_\xi(\hat{z}_t) - \text{sg}(V^{\lambda}_t)\right)^2\right]
\end{equation}

Here, $v_\xi(\hat{z}_t)$ is the critic's value, and $\text{sg}(V^{\lambda}_t)$
is the lambda-target with stopped gradients. This helps keep learning stable.

Some tricks are used to make the critic better:

\textbf{Target Network:} Like in DQN \cite{DQN}, a delayed copy of the critic is used for targets, updated every 100 steps.

\textbf{Trajectory Weighting:} Loss terms are weighted by predicted discounts, so episode ends are handled properly.

\textbf{Latent States:} The critic works on compact latent states, not raw observations, which makes learning faster and more stable.

The critic is usually a small MLP with ELU activations and about 1 million
parameters, outputting a single value for each state.

Given the advantages of model based RL and the effectiveness of the Dreamer
architecture we now come to the question of how to best represent the state of
the environment.

\section{Object-Centric Reinforcement Learning}
\label{sec:object_centric_rl}

Object-Centric Reinforcement Learning (OCRL) is a collection of methods that do
not make predictions on the raw pixel input but rather try to first create an
abstract representation of the environment which has some semantic
understanding of the objects within it. There are methods that use the pixel
input to create objects from it \cite{locatello2020object} but also approaches
where the objects states are extracted from RAM \cite{delfosse2023ocatari} or
given directly by the environment itself like in JAXAtari.

This object extraction mimics human perception, which naturally segments scenes
into distinct entities and focuses on their interactions
\cite{spelke1990principles}. Therefore object centricity provides several
advantages over pixel based methods.

\textbf{Compositional Understanding:} Object-centric representations naturally excel at treating input features as compositions of distinct entities.
This compositionality allows agents to reason about individual objects and their interactions by for example allowing the agent
to make connections between velocity and the position of a ball to a paddle.

\textbf{Improved Generalization:} By focusing on objects rather than pixel patterns, agents can generalize better across visually different but similar scenarios. For example agents that
understands the concept of a "ball" an an abstract object can use this knowledge in environments where the ball has a different shape or appearance in general \cite{zambaldi2018deep}.

\textbf{Enhanced Interpretability:} Object-centric representations provide naturally a better interpretability than pixel based representations since the agent can reason better that it perceives things as objects rather than "random" pixels.

\textbf{Sample Efficiency:} The structured nature of object-centric representations often leads to more sample-efficient learning. By working with compact, meaningful features rather than high-dimensional
pixel data, agents can learn policies with fewer rollouts. This is possible because the object space is already a kind of latent space. \cite{locatello2020object}.

A significant development in this field is the introduction of OCAtari
(Object-Centric Atari) by Delfosse et al. \cite{delfosse2023ocatari}. OCAtari
extends the widely-used Arcade Learning Environment (ALE) by providing
resource-efficient extraction of object-centric states for Atari 2600 games.
This framework fixes a gap, where despite growing interest in object-centric
approaches, no standardized benchmark existed for evaluating such methods on
the popular Atari domain.

The OCAtari framework works by two ways of extracting object states. It can
either extract the object states directly from the emulator's RAM or by using
template matching on the rendered frames. The RAM-based extraction is more
efficient and accurate. Directly having those extracted object states allows
for significantly faster training times and also supports researches in making
comparable findings since everyone can use the same object states.

The importance of object-centric approaches in Atari environments is especially
present in light of pixel-based methods in these domains. Delfosse et al.
demonstrated that deep RL agents without interpretable object-centric
representations can learn misaligned policies even in simple games like Pong
\cite{delfosse2023ocatari}. This misalignment problem strengthens the argument
for using object-centric understanding rather than attempting to retrofit
interpretability onto pixel-based systems.

Building upon OCAtari, JAXAtari \cite{machado2023revisiting} provides a
JAX-based implementation that offers additional performance benefits through
just-in-time compilation and vectorized environments. JAXAtari maintains
compatibility with the object-centric state extraction capabilities of OCAtari
while providing enhanced computational efficiency for large-scale experiments.
This framework is particularly relevant for our work as it enables efficient
training of \worldmodel{}s on object-centric representations while maintaining
the standardized benchmarking capabilities essential for reproducible research.

Object-centric representations naturally lend themselves to relational
reasoning, where agents must understand not just individual objects but also
the relationships and interactions between them. This capability is crucial for
complex decision-making in multi-object environments where the optimal policy
depends on understanding how different entities influence each other.

\chapter{Methodology}
\label{chap:methodology}

In this chapther we explore the actual implementation of the \worldmodel{}
along with the actor critic which learns in the imagined rollouts generated by
the \worldmodel{}. The implementation is based on the DreamerV2 architecture
\cite{hafner2020dream} described in Section \ref{sec:dreamer_architecture}.
Moreover we chose the game pong from the Atari benchmark suite to exemplify our
approach since it is a well understood environment and not too complex to
train on. The complete implementation including all training scripts and
evaluation tools is publicly available \cite{ocwmrl2025}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{images/Pong.jpg}
	\caption{Overview of the Pong \worldmodel{} architecture. The model takes in the current object-centric state and action, processes them through a deep residual MLP, and predicts the next object-centric state.}
	\label{fig:pong_architecture}
\end{figure}

\section{\worldmodel{} Architecture} \label{sec:world_model_arch}

Unlike DreamerV2 which learns on images we leverage the fact that we already
have object centric states. So instead of a 84 by 84 Pixel image as input we have
a 12 object states. Which have the following features: Each object-centric
state in Pong is represented as a \texttt{PongObservation} containing:
\begin{itemize}
	\item \textbf{Player Paddle:} Position (x, y), width, height
	\item \textbf{Enemy Paddle:} Position (x, y), width, height
	\item \textbf{Ball:} Position (x, y), width, height
	\item \textbf{Player Score:} Single scalar value
	\item \textbf{Enemy Score:} Single scalar value
\end{itemize}

\subsection{Deep Residual MLP (PongMLPDeep)}
\label{subsec:pongmlp}

Our \worldmodel{} is a deep feedforward neural network. Instead of using
recurrent architectures, we rely on frame stacking to provide the temporal
context. The input state contains 4 consecutive frames, which gives the model
information about velocities and trajectories through the temporal differences
between frames.

Using a simple feedwordward network has several advantages. First, they are
significantly faster to train and evaluate than recurrent models, since they
have no sequential dependencies that require iterative computation. Also using
them during training becomes easier since feeding them a single frame is sufficient
which allows for better parallelization. Second, the structured nature
of object-centric states (explicit positions) means the model does
not need to learn temporal abstractions from scratch. The main disadvantage
of feedforward models is that they lack temporal context which is solved to some degree
by the frame stacking.

Our implementation uses a 4-layer residual architecture that predicts
frame-shifted states meaning the model only predicts a single frame meaning 12
output features and just reuses the last 3 frames of the input in the first 3
frames of the input. The model is defined as:

\begin{equation}
	\hat{s}_{t+1} = f_\theta(s_t, a_t)
\end{equation}

where $s_t \in \mathbb{R}^{48}$ is the normalized object-centric state
containing 4 stacked frames, $a_t \in \{0,1,2,3,4,5\}$ is the discrete action,
and $f_\theta$ represents our parameterized dynamics function represented by
the aforementioned feedwordward network. Importantly, the output
$\hat{s}_{t+1}$ represents a shifted frame stack: With the input frames $[f_0,
			f_1, f_2, f_3]$ and output frames $[f_1, f_2, f_3, f_{\text{new}}]$ where the
oldest frame $f_0$ gets discarded and a new predicted frame gets appended.

The architecture consists of following layers with hidden dimension $h = 256
	\cdot \text{scale\_factor}$:

\begin{enumerate}
	\item \textbf{Input Processing:} State and one-hot encoded action are concatenated: $x_0 = [s_t; \text{OneHot}(a_t)] \in \mathbb{R}^{54}$
	\item \textbf{Layer 1:} $x_1 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_0)))$
	\item \textbf{Layer 2 with Residual:} $x_2 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_1))) + x_1$
	\item \textbf{Layer 3 with Residual:} $x_3 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_2))) + x_2$
	\item \textbf{Layer 4 with Residual:} $x_4 = \text{GELU}(\text{LayerNorm}(\text{Dense}_h(x_3))) + x_3$
	\item \textbf{Output Layer:} Predict only the new frame: $f_{\text{new}} = \text{Dense}_{12}(x_4)$
	\item \textbf{Frame Shifting:} Construct $\hat{s}_{t+1}$ by shifting frames: $[f_1, f_2, f_3, f_{\text{new}}]$
\end{enumerate}

We use residual connections, layer normalization, and GELU activations to
stabilize training and prevent vanishing gradients in the deep network.

An important architectural choice is to predict only the newest frame instead
of the entire 48-dimensional state. This reduces the output complexity from 48
to 12 dimensions and naturally incorporates an inductive bias: the model
focuses on predicting how objects change instead of reconstructing static
features from previous frames.

\subsection{State Normalization and Stability}
\label{subsec:normalization}

Normalization is crucial for many machine learning models. In the case of pong
it is especially important since the different features have very different
scales. Therefore we apply feature-wise normalization to the object-centric
states:

\begin{equation}
	\tilde{s}_t = \frac{s_t - \mu}{\sigma}
\end{equation}

where $\mu \in \mathbb{R}^{48}$ and $\sigma \in \mathbb{R}^{48}$ are the mean
and standard deviation computed over the training dataset.. The normalization
statistics are computed only from valid training transitions (excluding the
episode boundaries) and get saved with the model checkpoint for the consistent
inference.

During training, we normalize the inputs before feeding them to the network,
and denormalize the predictions back to the original scale:

\begin{equation}
	\tilde{s}_{t+1} = \hat{s}_{t+1} \cdot \sigma + \mu
\end{equation}

For the \worldmodel{} training, we additionally apply feature-specific loss
weighting to emphasize the physically critical variables:

\begin{equation}
	\mathcal{L}_{\text{world}} = \mathbb{E}\left[\sum_{i=1}^{48} w_i \left(\hat{s}_{t+1}^{(i)} - s_{t+1}^{(i)}\right)^2\right]
\end{equation}

where the weights are set as: $w_i = 10.0$ for ball position features (indices
32-39), $w_i = 5.0$ for ball velocity features (indices 16-23), $w_i = 2.0$ for
paddle position (indices 4-7), and $w_i = 1.0$ for other features. This is
useful to teach the model to focus on accurately predicting the ball dynamics,
which are crucial for proper reward prediction and policy learning.

\section{Actor-Critic Integration}
\label{sec:actor_critic}

Following the DreamerV2 framework, we train an actor-critic policy entirely
from the imagined rollouts generated by our object-centric \worldmodel{}.
Different to the original DreamerV2 which uses latent states from a learned
image encoder, our approach uses the object-centric states from JAXAtari. This
architecture avoids the need for the representation learning while maintaining
the sample efficiency benefits of model based RL.

\subsection{DreamerV2-Style Actor-Critic}
\label{subsec:dreamer_ac}

\textbf{Actor Network:} The actor $\pi_\psi(a|s)$ is a categorical distribution over the 6 discrete actions in Pong (NOOP, UP, DOWN, and their variants). It is implemented as a 3-layer MLP with 64 hidden units per layer, ELU activations, and a final softmax output:

\begin{align}
	h_1           & = \text{ELU}(\text{Dense}_{64}(s))        \\
	h_2           & = \text{ELU}(\text{Dense}_{64}(h_1))      \\
	h_3           & = \text{ELU}(\text{Dense}_{64}(h_2))      \\
	\pi_\psi(a|s) & = \text{Categorical}(\text{Dense}_6(h_3))
\end{align}

\textbf{Critic Network:} The critic $v_\xi(s)$ estimates the state values through a distributional representation. Instead of outputting a single scalar value, it predicts a Gaussian distribution $\mathcal{N}(\mu_v, \sigma_v)$ over the returns:

\begin{align}
	h_1           & = \text{ELU}(\text{Dense}_{64}(s))        \\
	h_2           & = \text{ELU}(\text{Dense}_{64}(h_1))      \\
	h_3           & = \text{ELU}(\text{Dense}_{64}(h_2))      \\
	\mu_v         & = \text{Dense}_1(h_3)                     \\
	\log \sigma_v & = \text{clip}(\text{Dense}_1(h_3), -3, 0)
\end{align}

The distributional critic provides the uncertainty estimates and has been shown
to improve the learning stability in actor-critic methods \cite{yue2020implicit}.
During training, we minimize the negative log-likelihood of the target returns
under this distribution.

\subsection{Policy Learning in Imagined Rollouts}
\label{subsec:imagined_rollouts}

A core principle of model based RL is to learn policies from the simulated
trajectories instead of requiring expensive real environment interactions. Our
training procedure operates in two phases:

\textbf{Phase 1: Rollout Generation}

Given a batch of $N=30000$ initial states sampled from the experience buffer,
we generate the imagined trajectories of length $H=7$ by repeatedly applying
the current actor policy in the \worldmodel{}:

\begin{algorithm}
	\caption{Imagined Rollout Generation}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} Initial states $\{s_0^{(i)}\}_{i=1}^N$, \worldmodel{} $f_\theta$, actor $\pi_\psi$, critic $v_\xi$
		\STATE \textbf{Output:} Trajectories $(s_t, a_t, r_t, \gamma_t, v_t)$ for $t=0..H$
		\FOR{$i = 1$ to $N$}
		\STATE $s_t \gets s_0^{(i)}$ \hfill // Initialize from experience buffer
		\FOR{$t = 0$ to $H-1$}
		\STATE Sample action: $a_t \sim \pi_\psi(\cdot | s_t)$
		\STATE Predict value: $v_t \gets \mathbb{E}[v_\xi(s_t)]$
		\STATE Predict next state: $s_{t+1} \gets f_\theta(s_t, a_t)$
		\STATE Compute reward: $r_t \gets R(s_{t+1}, a_t)$
		\STATE Set discount: $\gamma_t \gets 0.95$
		\ENDFOR
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

The rollout length $H=7$ was chosen to balance the \worldmodel{} accuracy with
providing the agent with sufficient horizon to plan steps. Longer rollouts ($H
	> 10$) accumulated the prediction errors that degraded the policy learning.

Critically, the rewards are computed from the predicted states using a
hand-crafted reward function instead of relying on the environment's score
since its prediction are too inaccurate within the model generated rollouts.

\begin{equation}
	R(s_{t+1}, a_t) = \begin{cases}
		+1.0 & \text{if ball crosses left edge and enemy paddle misaligned} \\
		-1.0 & \text{if ball crosses right edge}                            \\
	\end{cases}
\end{equation}

The little addition of the enemy paddle misaligned helps in reducing spurious
rewards when the ball is actually properly blocked by the enemy paddle.

\textbf{Phase 2: Policy Optimization}

Once the imagined rollouts are generated, we train the actor and critic using
the collected trajectories. The critic is trained to predict the lambda-returns
(described in Section \ref{subsec:lambda_returns}), while the actor maximizes
these predicted returns using the REINFORCE gradient estimator.

A key implementation detail is the batching strategy. We flatten all
trajectories into a single dataset of $(s_t, a_t, V^\lambda_t)$ tuples totaling
$N \times H = 210,000$ samples, then we perform mini-batch SGD with batch size. This approach improves the hardware utilization and gradient stability
compared to the per-trajectory updates.

\subsection{Lambda-Return Computation}
\label{subsec:lambda_returns}

The lambda-return $V^\lambda_t$ provides a bias-variance trade-off between the
Monte Carlo returns (high variance, low bias) and the TD(0) estimates (low
variance, high bias). We compute the lambda-returns recursively following the
DreamerV2 formulation:

\begin{equation}
	V^\lambda_t = r_t + \gamma_t \cdot \left[(1 - \lambda) v_\xi(s_{t+1}) + \lambda V^\lambda_{t+1}\right]
	\label{eq:lambda_return}
\end{equation}

with the base case $V^\lambda_H = v_\xi(s_H)$ at the horizon. We set $\lambda =
	0.95$ following the DreamerV2's Atari configuration, which heavily weights the
bootstrapped value estimates while maintaining some Monte Carlo influence.

The computation is implemented via a JAX scan operation that processes the
trajectories backwards in time. These lambda-returns serve as regression targets for the critic.

\begin{algorithm}
	\caption{Lambda-Return Computation (Backward Pass)}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} Rewards $\{r_t\}_{t=1}^{H}$, values $\{v_t\}_{t=0}^{H}$, discounts $\{\gamma_t\}_{t=1}^{H}$, $\lambda$
		\STATE \textbf{Output:} Lambda-returns $\{V^\lambda_t\}_{t=0}^{H-1}$
		\STATE $V^\lambda_H \gets v_H$ \hfill // Bootstrap from final value
		\FOR{$t = H-1$ down to $0$}
		\STATE $V^\lambda_t \gets r_{t+1} + \gamma_{t+1} \left[(1-\lambda) v_{t+1} + \lambda V^\lambda_{t+1}\right]$
		\ENDFOR
		\STATE \textbf{return} $\{V^\lambda_t\}_{t=0}^{H-1}$
	\end{algorithmic}
\end{algorithm}

\begin{equation}
	\mathcal{L}_{\text{critic}} = \mathbb{E}_{s \sim \text{rollouts}}\left[-\log p_{v_\xi}(V^\lambda | s)\right]
\end{equation}

where $p_{v_\xi}(V^\lambda | s)$ is the Gaussian density predicted by the
critic. The stop-gradient operator is applied to the lambda-returns to prevent
the gradients from flowing back through the target computation.

The actor is trained using the policy gradient with advantages computed as $A_t
	= V^\lambda_t - v_\xi(s_t)$:

\begin{equation}
	\mathcal{L}_{\text{actor}} = \mathbb{E}_{s,a \sim \text{rollouts}}\left[-\log \pi_\psi(a|s) \cdot \text{sg}(A_t) - \eta \mathcal{H}[\pi_\psi(\cdot|s)]\right]
\end{equation}

where $\eta = 0.01$ is the entropy bonus coefficient that encourages
exploration, and $\text{sg}(\cdot)$ denotes the stop-gradient. We additionally
normalize the advantages to have zero mean and unit variance within each batch,
which improves the gradient stability.

\section{Training Pipeline}
\label{sec:training_pipeline}

Our complete training procedure alternates between the \worldmodel{} training
and the policy optimization, with periodic retraining of the \worldmodel{}
using fresh experience collected by the improving policy. This cyclic approach
is in many instances of model based RL incredible important since the
\worldmodel{} needs proper and diverse data to learn from to also predict later
stages of the game accurately to avoid out of distribution states. This is less
critical in Pong since the state space is relatively small, but in more complex
games this becomes essential.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.3\textwidth]{images/trainingschedule.png}
	\caption{Training schedule showing the alternating phases of \worldmodel{} training and policy optimization.}
	\label{fig:trainingschedule}
\end{figure}

\subsection{\worldmodel{} Training and Experience Collection} \label{subsec:world_model_training}

\begin{algorithm}
	\caption{Complete Training Pipeline}
	\begin{algorithmic}[1]
		\STATE \textbf{// Phase 1: Initial Setup}
		\IF{no experience buffer exists}
		\STATE Collect 160,000 steps using ball-tracking policy in real environment
		\STATE Train \worldmodel{} $f_\theta$ for 50 epochs on collected data
		\ENDIF
		\STATE Load \worldmodel{} parameters $\theta$ and experience buffer
		\STATE Initialize actor $\pi_\psi$ and critic $v_\xi$ with random weights
		\STATE
		\STATE \textbf{// Phase 2: Iterative Training}
		\FOR{iteration $i = 0$ to 3120}
		\STATE \textbf{// Policy optimization from imagination}
		\STATE Sample 30,000 initial states from experience buffer
		\STATE Generate $H=7$ step imagined rollouts using $f_\theta$, $\pi_\psi$
		\STATE Compute lambda-returns using $v_\xi$
		\STATE Update $\pi_\psi$ and $v_\xi$ for 10 epochs on imagined data
		\STATE
		\STATE \textbf{// \worldmodel{} retraining (for model-based mode only)}
		\IF{$i \mod 10 = 0$ and using model-based rollouts}
		\STATE Collect 160,000 new steps using current $\pi_\psi$ in real environment
		\STATE Retrain $f_\theta$ for 50 epochs on updated experience buffer
		\ENDIF
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

The training pipeline begins with initially collection experience using a pre
defined heuristic policy, followed by alternating phases of \worldmodel{}
training and policy optimization. This policy moves the paddle toward the
ball's y-coordinate with 50\% random exploration, which provides reasonable
coverage of reachable states without requiring a pre-trained agent. The
ball-tracking policy achieves pretty low reward but sometimes scores a ball
which is sufficient for learning proper environment dynamics.

The collected experience is stored as tuples $(s_tTraining uses the Adam optimizer with, a_t, s_{t+1})$ with
metadata marking the episode and life boundaries (ball resets). These
boundaries help in training the \worldmodel{} to avoid learning spurious
transitions across them.

\textbf{\worldmodel{} Training Details:} The \worldmodel{} is trained via supervised learning to predict the one-step transitions. We use the mean squared error loss with feature-specific weighting:

\begin{equation}
	\mathcal{L}_{\text{world}} = \mathbb{E}_{(s,a,s') \sim \mathcal{D}}\left[\sum_{i=1}^{48} w_i \left(f_\theta(s, a)^{(i)} - s'^{(i)}\right)^2\right]
\end{equation}

\textbf{Experience Buffer Management:} The experience buffer maintains a sliding window of recent transitions. In the real-environment baseline mode, we do not update the buffer after the initial collection. In the model-based mode, we periodically collect fresh data using the current policy, then retrain the \worldmodel{} every 10 policy optimization iterations.

When retraining, we compeletely resample transitions from the updated buffer to
ensure the \worldmodel{} learns from the latest distribution of states visited
by the improving policy.

\subsection{Policy Optimization}
\label{subsec:policy_optimization}

Within each training iteration, the policy optimization proceeds through
several stages:

\textbf{Stage 1: Rollout Generation:} We sample initial states uniformly from the experience buffer and generate the 7-step imagined trajectories. The large batch size improves the GPU utilization and provides diverse training signal. The rollouts are generated in sub-batches to manage the memory, with each sub-batch processed in parallel via the JAX's vmap and scan primitives.

\textbf{Stage 2: Data Preparation:} The generated trajectories are flattened into a dataset of $(s_t, a_t, r_t, \gamma_t, v_t)$ tuples. We compute the lambda-returns via a vectorized backward pass over all trajectories simultaneously. The advantages are then computed and normalized.

\textbf{Stage 3: Actor-Critic Updates:} We perform 10 epochs of mini-batch gradient descent on the prepared data. Each epoch shuffles the data and processes it in batches. The critic and actor get updated sequentially within each mini-batch:

\begin{align}
	\xi  & \gets \xi - \alpha_{\text{critic}} \nabla_\xi \mathcal{L}_{\text{critic}} \\
	\psi & \gets \psi - \alpha_{\text{actor}} \nabla_\psi \mathcal{L}_{\text{actor}}
\end{align}



\chapter{Experiments and Results}
\label{chap:experiments}

This chapter shows the experiments of the object-centric model based
reinforcement learning approach. We do three experiments that compare
object-centric \worldmodel{}s against pixel-based baselines. We look at
prediction accuracy, sample efficiency, and robustness to visual changes.

\section{Experimental Setup}
\label{sec:exp_setup}

All experiments are done in the Pong environment. For the object-centric
approach, we use JAXAtari to extract states. This gives us direct access to
object positions and dimensions. This includes player paddle position (x, y),
dimensions (width, height), enemy paddle position and dimensions, and ball
position and dimensions. We use frame stacking with 4 frames, so we get a
48-dimensional state vector after we remove score features. The reward
prediction is omitted in our setup as the reward can be directly read out from
the game state, which is common in Atari games when using object-centric
approaches that encompass the score.

The pixel-based DreamerV2 baseline uses the standard Atari configuration with
84 by 84 greyscale observations.

Training uses the Adam optimizer with learning rate $3 \times 10^{-4}$, batch
size 512, and 50 epochs with learning rates $\alpha_{\text{critic}} = 5 \times 10^{-4}$ and
$\alpha_{\text{actor}} = 8 \times 10^{-5}$. Both optimizers use the gradient
clipping at global norm 0.5 to prevent the instability. The model gets saved every 10 epochs along with the
normalization statistics.



\textbf{Evaluation Metrics:} We measure \worldmodel{} performance using mean
squared error (MSE) over prediction horizons from 1 to 20 steps. We convert both models'
predictions to 84×84 grayscale images to make comparison fair. Sample
efficiency is measured by the environment steps that are required to reach high episode
returns. We test robustness through pixel inversion (transforming pixel
value $p$ to $255 - p$). This changes background colors but preserves
object boundaries.

Because of computational constraints, we report single-seed results for Dreamer
and three seeds for the object-centric approach. The trends are consistent
across seeds.

\section{\worldmodel{} Prediction Accuracy} \label{sec:world_model_perf}

We compare prediction accuracy of object-centric and pixel-based \worldmodel{}s
using a standardized protocol. Both models do 20-step autoregressive rollouts
from 10 randomly sampled initial states with the same action sequences. We
convert predictions to 84×74 grayscale images for fair comparison. We use this
resolution because the top of the image is cropped to remove the score display
in Pong.

Figure \ref{fig:prediction_comparison} shows the results. It shows clear
advantages for the object-centric approach across all horizons.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{images/model_comparison_plot.png}
	\caption{\worldmodel{} prediction error comparison over multi-step rollouts. The JAX object-centric \worldmodel{} (orange) exhibits substantially lower MSE compared to DreamerV2 (blue) across all prediction horizons from 1 to 20 steps.}
	\label{fig:prediction_comparison}
\end{figure}

At single-step prediction, the object-centric model achieves MSE of
approximately 2-6, while DreamerV2 exhibits 45-60. At the 20-step horizon, the
object-centric model has MSE below 20, while DreamerV2's error grows to 60-90.
When we average over all horizons, the object-centric approach achieves mean
MSE of approximately 10 compared to DreamerV2's 65. This is an 85\% reduction
in prediction error.

Object centric predictions show smoother trajectories and a more gradual
increase in prediction error. DreamerV2 predictions show more variability, with
higher MSE. This is partly because of the challenge of pixel-level
reconstruction from latent states. The consistent advantage across prediction
horizons shows that the object-centric model has learned stable dynamics that
respect Pong's physics. Although benefecial this should not be taken out of context
since we are using already premade object centric states from JAXAtari which
makes the learning task significantly easier. Therefore there is a natural advantage
especially in rendering are pixel based inaccuracies are just not possible with object centric states 
that operate on integers.

These results show a big improvement in prediction accuracy. The improved
accuracy makes longer imagination horizons possible without too much error
accumulation. This supports reliable multi-step policy learning as described in
Section \ref{subsec:imagined_rollouts}.

\section{Policy Learning and Sample Efficiency}
\label{sec:policy_results}

We evaluate actor-critic training on imagined rollouts from the object-centric
\worldmodel{}. We compare learning efficiency and final performance against
DreamerV2.

Figure \ref{fig:training_comparison} shows learning curves. It plots episode
return against cumulative environment steps. This shows big differences in
sample efficiency.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{images/training_curve.png}
	\caption{Training progress comparison between object-centric (blue) and DreamerV2 (orange). The object-centric approach achieves high returns (extracted from real rollouts) with an order of magnitude fewer environment interactions.}
	\label{fig:training_comparison}
\end{figure}

The object-centric approach shows rapid learning in early training. Starting
from returns around -20, the model quickly improves to above +15 within
100,000-200,000 environment steps. DreamerV2 shows gradual learning over 5.5
million steps. It requires 1-2 million steps to achieve similar performance
levels. This is an order of magnitude difference in sample efficiency.

Despite the big differences in sample efficiency, both approaches converge to
similar final performance (15-21 return range). The object-centric approach
shows rapid early convergence followed by a stable plateau, while DreamerV2
shows longer learning with occasional fluctuations. Both eventually learn
effective Pong policies that achieve near-optimal play.

The successful policy learning from object-centric imagined rollouts shows that
DreamerV2-style actor-critic can learn optimal policies from simulated
experiences. The rapid learning shows that imagined rollouts provide meaningful
training signal. The stable performance suggests the actor-critic avoids
overfitting to \worldmodel{} errors. The 7-step imagination horizon is
sufficient for effective policy learning with appropriate temporal credit
assignment.

The efficiency gains can be because of superior prediction accuracy (Section
\ref{sec:world_model_perf}), compact 48-dimensional state representation that
makes faster learning possible, a shaped reward and explicit object features providing natural
inductive bias. The object-centric approach can reach strong performance within
100,000-200,000 steps. This places it among the most sample-efficient methods
for this domain.

\section{Robustness to Visual Perturbations}
\label{sec:robustness}

To test if pixel-based models learn robust object-level understanding, we
evaluate DreamerV2's performance under pixel inversion. This maps each pixel
value $p$ to $255 - p$. This transformation changes background colors but
preserves edges, object boundaries, and game dynamics. A model with real
object-centric representations should be invariant to such superficial changes,
while models that rely on specific pixel patterns should show significant
degradation.

We evaluate the trained DreamerV2 agent under normal and inverted visual
conditions. We collect 20 episodes in each condition without retraining.

\begin{table}[ht]
	\centering
	\caption{DreamerV2 performance under normal and inverted visual conditions over 20 evaluation episodes.}
	\label{tab:robustness_results}
	\begin{tabular}{lcccc}
		\toprule
		Condition & Mean Return & Std Dev & Min Return & Max Return \\
		\midrule
		Normal    & 19.15       & 1.19    & 17.0       & 21.0       \\
		Inverted  & -20.80      & 0.52    & -21.0      & -20.0      \\
		\bottomrule
	\end{tabular}
\end{table}

Under normal conditions, DreamerV2 achieves $19.15 \pm 1.19$ mean return, which
shows effective policy learning. With inversion, performance collapses
completely to $-20.80 \pm 0.52$, with 19 of 20 episodes achieving -21 (worst
possible). This is a 209\% degradation, going from near-optimal to performance
similar to random actions.

The consistency of failure across all episodes and minimal variance in the
inverted condition show systematic misperception rather than occasional errors.
The agent does a consistent but fundamentally wrong strategy, which suggests
the visual perturbation causes misidentification of critical game elements.

These results show that DreamerV2 builds on specific pixel-level features
rather than learning object-centric representations. Despite high performance
within the training distribution, the agent fails to extract representations of
game entities. This suggests another advantage of our object-centric approach.
Agents which operate on symbolic object representations (paddle positions, ball
coordinates) achieve invariance to visual changes by construction. They encode
only relevant geometric and semantic properties rather than pixel values
assuming proper object extraction and thus our method is agnostic to such distribution 
shifts.


\section{Summary of Findings}
\label{sec:summary}

In this final section we summarize the key findings from our experiments. We
directly address the research questions posed in Chapter
\ref{chap:introduction}.

\textbf{RQ1: How do object-centric \worldmodel{}s compare to pixel-based \worldmodel{}s
	regarding prediction accuracy and sample efficiency?}

The prediction accuracy experiment (Section \ref{sec:world_model_perf}) shows
clear superiority of object-centric \worldmodel{}s. The object-centric approach
achieves 85\% reduction in mean prediction error (MSE $\approx$10 vs.
$\approx$65) with single-step errors of 2-6 compared to DreamerV2's 45-60. This
advantage persists across all 20 prediction horizons.

For sample efficiency, the policy learning experiment (Section
\ref{sec:policy_results}) shows the object-centric approach reaches high
performance (+15 return) within 100,000-200,000 environment steps compared to
DreamerV2's 1-2 million steps. This is an order of magnitude improvement. Both
methods achieve similar final performance (15-21 return range). This confirms
that improved \worldmodel{} accuracy translates directly into dramatically
reduced data requirements.

\textbf{RQ2: Can an actor-critic framework be effectively trained on simulated
	rollouts from object-centric \worldmodel{}s?}

The policy learning experiment answers this question positively. The
DreamerV2-style actor-critic successfully learned optimal policies entirely
from 7-step imagined rollouts. It achieved rapid convergence and stable
performance without overfitting to \worldmodel{} errors. This shows that
standard actor-critic methods work well with object-centric \worldmodel{}s
without requiring specialized architectural modifications.

\textbf{RQ3: Does the object-centric inductive bias in \worldmodel{}s improve
	robustness to changes in the environment?}

The pixel inversion experiment (Section \ref{sec:robustness}) addresses this
question by testing whether learned representations capture robust object-level
understanding. DreamerV2's performance collapsed from 19.15 to -20.80 mean
return (209\% degradation) under pixel inversion. This shows reliance on for
humans irrelevant visual features. Object-centric approaches achieve robustness
by construction through symbolic representations which do not noticeably change
with visual styling assuming robust object extraction.

\begin{table}[ht]
	\centering
	\caption{Summary of key experimental results mapped to research questions.}
	\label{tab:summary_results}
	\begin{tabular}{lcc}
		\toprule
		Metric (Research Question)             & Object-Centric & DreamerV2   \\
		\midrule
		\textbf{RQ1: Prediction \& Efficiency} &                &             \\
		1-step MSE (84×74 grayscale)           & 2-6            & 45-60       \\
		Mean MSE (1-20 steps)                  & $\approx$10    & $\approx$65 \\
		Steps to reach +15 return              & 100k-200k      & 1M-2M       \\
		\midrule
		\textbf{RQ2: Actor-Critic Integration} &                &             \\
		Final performance (mean return)        & 15-21          & 15-20       \\
		Policy learning success                & Yes            & Yes         \\
		\midrule
		\textbf{RQ3: Robustness}               &                &             \\
		Performance under pixel inversion      & 15-21*           & -20.80      \\
		Performance degradation                & 0*           & 209\%       \\
		\bottomrule
	\end{tabular}
	\\ \vspace{0.2cm}
	{\footnotesize *Object-centric approach is invariant to visual changes by construction.}
\end{table}

\chapter{Discussion}
\label{chap:discussion}

In this chapter we discuss how the experimental results can be interpreted and
what technical insights we can draw from our design choices. We also look at
the limitations of our approach.

\section{Interpretation of Results}
\label{sec:interpretation}

The object-centric approach shows significant advantages across prediction
accuracy, sample efficiency, and robustness. The 85\% reduction in prediction
error can be because of several interrelated factors. First, we have reduced
feature size (12 features per frame versus 12,288 pixels). Second, the
inductive bias aligns with Pong's fundamentally object-based dynamics. Third,
we eliminate the reconstruction burden that is required by pixel-based
decoders. Also, we have continuous-state dynamics where small prediction errors
correspond to small positional offsets with limited downstream consequences.
This is different from discrete-state games where binary prediction errors
fundamentally alter game states.

The order-of-magnitude improvement in sample efficiency (100k-200k versus 1-2M
steps) comes from multiple mechanisms. First, we have superior \worldmodel{}
accuracy which enables higher-quality simulated experiences. Second, the
compact 48-dimensional state representation reduces function approximation
complexity. Third, semantic feature availability allows immediate leverage
without extensive exploration.

The pixel inversion experiment revealed critical brittleness in pixel-based
representations. DreamerV2's complete collapse (from +19 to -21) with
consistent failure across all episodes shows systematic misperception. The
agent relies on specific color patterns and brightness values that reverse
under inversion. The latent representation optimized for reconstruction does
not encourage invariance to color transformations. It encodes brightness and
RGB values even though they are irrelevant to game semantics. This shows a
broader challenge. Pixel-based agents may achieve high performance on specific
visual instantiations while failing to generalize to semantically equivalent
variants. Object-centric approaches achieve robustness by construction through
symbolic representations that are invariant to visual styling.

\section{Technical Insights and Design Choices}
\label{sec:technical_insights}

Key design decisions include several things. First, we use a feedforward deep
residual MLP architecture with frame stacking for temporal context (4
consecutive frames enable velocity inference from positional differences).
Second, we use single-frame prediction with frame shifting to reduce output
complexity. Third, we use feature-specific loss weighting that emphasizes ball
position and velocity (weights 10.0 and 5.0 versus 1.0 for other features).
This proved essential for accurate dynamics learning.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{images/training_curve_seed_comparison.png}
	\caption{Comparing training with and without feature weighting. While both approaches eventually converge to similar performance levels, the variant without feature weighting shows slightly worse performance in the initial training phase before reaching comparable final scores.}
	\label{fig:feature_weighting_ablation}
\end{figure}

To validate the importance of feature weighting, we conducted an ablation study comparing training with and without this design choice (Figure \ref{fig:feature_weighting_ablation}). The results show that while feature weighting is not strictly essential for learning, it does provide benefits during early training. The agent trained without feature weighting exhibits slightly worse initial performance but eventually converges to similar final scores. This suggests that feature weighting helps guide the model to focus on the most critical dynamics (ball trajectory) early in training, accelerating convergence, though the agent can still learn effectively without it.

A significant challenge was reward prediction within imagined rollouts. Score
prediction proved unreliable because of extremely imbalanced targets (mostly 0
changes, rare ±1 at specific moments). We implemented a handcrafted geometric
reward function based on predicted ball position and paddle alignment. This
provides smooth learnable signal that correlates with actual reward structure.
This trades generality for reliability but shows the need for future work on
learned reward models.

The necessity for a carefully handcrafted reward also raises some discussions.
A possible reason why DreamerV2 may suceed to directly predict the rewards may be that the 
latent space representation is more flexible and contains more information that can be optimized
by the model to accurateley predict the rewards. That lies in harsh contrast to our object centric representation
which is fixed and may not contain all the necessary information to accurately predict the rewards. 
Furthermore object centric representations may discard some information that is necessary to predict the rewards
accurately. This highlights a trade-off between using fixed object centric representations that are
excellent for dynamics modeling but may lack the flexibility for reward prediction versus learned latent representations
that can adapt to the task at hand. This suggests future work on pinpointing exactly why there are such differences.

Training stability depended critically on several things. First, we use
feature-wise normalization to handle vastly different scales across position,
dimension, and velocity features. Second, we use advantage normalization for
policy gradient stability. Third, we carefully select the imagination horizon
(7-step balances temporal context against error accumulation). The JAX
implementation with JIT compilation and vectorization provided substantial
computational benefits. This is through parallel rollout generation across
30,000 initial states and efficient scan-based lambda-return computation.
Compact 48-dimensional representations make single-GPU training feasible
compared to memory-intensive pixel-based convolutional architectures.

\section{Limitations and Constraints}
\label{sec:limitations}

The 7-step imagination horizon is a short planning window. It constrains the
approach to environments where limited temporal context suffices. Many RL tasks
benefit from long-term planning over tens or hundreds of steps. This is notably
shorter than DreamerV2's typical 15-step horizon for Atari games. Several factors may contribute 
to this difference. Our
model exhibits some instability in certain game states, particularly ball behavior
near edges and during respawn events. DreamerV2's RSSM architecture includes a
mechanism where predicted latent states are refined by encoding actual observations
and comparing them via KL divergence, potentially providing implicit error correction
that our purely autoregressive approach does not provide. However, it is worth noting that
despite these limitations and the shorter horizon, our approach still achieves lower prediction errors overall, 
suggesting that the 7-step horizon may be sufficient for this domain. Future work on learned error
correction or a more probabilistic predictions could enable longer horizons.
This is not a limitation of the object-centric approach per se but rather of the current 
implementation as especially
the use of a \worldmodel{} that uses an autoregressive approach.

The approach fundamentally depends on reliable object extraction. JAXAtari
provides this through direct RAM access. But many real-world domains lack
explicit object representations. This requires learned extraction methods (slot
attention, MONet) that introduce additional complexity and potential failure
modes. This trades generality for performance. Pixel-based methods apply to any
visually observed environment while object-centric approaches achieve superior
performance only in structured domains.

Another limitation is the reliance on handcrafted reward functions for policy
learning. In contrast to that DreamerV2 successfully learns to predict rewards. This suggests that fixed object-centric representations, while excellent for
dynnamics modeling may abstract away crucial information for reward prediction.
Future work should explore why this happens precisely and whether different
representations or potentially learned object-centric features can avoid that limitation.


\section{Broader Implications}
\label{sec:broader_implications}

A central theme is the importance of inductive biases that align with task
structure. The object-centric representation encodes the assumption that
environments consist of discrete entities with evolving properties. This proves
correct for Pong and yields substantial advantages. This represents a
fundamental trade-off. Domain-general pixel-based methods sacrifice performance
for versatility, while domain-specific object-centric methods achieve
efficiency at the cost of scope. Object-centric representations are
particularly valuable when (1) reliable object extraction is available, (2)
dynamics are primarily determined by object interactions, (3) objects have
continuous state properties, and (4) sample efficiency is critical.

The object-centric approach offers interpretability advantages. \worldmodel{}
predictions are directly interpretable as object position and velocity
forecasts. This enables human verification. Actor decisions conditioned on
explicit object states facilitate analysis of which features influence action
selection. During development, transparency simplified debugging by making
prediction errors and policy failures immediately visible. This accelerates the
development cycle.

Important open questions concern scalability to complex domains. Pong contains
only 3 objects. Games like Breakout (dozens of bricks) or Seaquest (multiple
fish types) require handling variable numbers of objects with diverse types and
behaviors. This potentially requires graph neural networks or attention
mechanisms. Complex interaction types (pickup, destruction, transformation) and
partial observability (occlusion, object persistence) may require more
sophisticated architectures than the feedforward MLP we use here. This
highlights important directions for future research.

\chapter{Conclusion and Future Work}
\label{chap:conclusion}

This thesis investigated the integration of object-centric representations with
model-based reinforcement learning. It shows substantial advantages in
prediction accuracy, sample efficiency, and robustness compared to pixel-based
approaches. Through experiments in the Pong environment, we established that
object-centric \worldmodel{}s enable dramatically faster policy learning while
maintaining comparable final performance to state-of-the-art pixel-based
methods.

We developed a feedforward deep residual MLP architecture that operates
directly on symbolic object states extracted from JAXAtari. It incorporates
frame stacking for temporal context and feature-specific loss weighting. The
DreamerV2-style actor-critic successfully adapted to object-centric
\worldmodel{}s. It learns entirely from simulated rollouts and demonstrates
compatibility with modern model-based RL algorithms. We did comprehensive
experiments comparing against DreamerV2. This provided quantitative evidence
across prediction accuracy, sample efficiency, and robustness dimensions.

The object-centric \worldmodel{} achieved 85\% reduction in mean squared error
compared to DreamerV2 across prediction horizons from 1 to 20 steps. It has
single-step MSE of 2-6 versus 45-60 for the pixel-based baseline. The
object-centric approach reached high episode returns within 100,000-200,000
environment steps compared to DreamerV2's 1-2 million steps. This is an order
of magnitude improvement in sample efficiency. The actor-critic successfully
learned optimal policies entirely from 7-step imagined rollouts with rapid
convergence and stable performance. The pixel inversion experiment revealed
critical brittleness in pixel-based representations. DreamerV2's performance
collapsed from +19 to -21 mean return. This shows reliance on superficial
visual features rather than robust object-level understanding.

These results show that representation matters profoundly for learning
efficiency and robustness. The magnitude of observed differences (85\% error
reduction, 10× sample efficiency gain) shows how significant appropriate
representations can be. Success comes from alignment between representational
inductive bias and task structure. When domain structure is known or reliably
extractable, incorporating that structure into representations yields
substantial benefits. The order-of-magnitude improvement suggests that
combining object-centric representations with model-based methods offers a
promising path toward practical sample efficiency in domains where object
extraction is feasible. Object-centric representations provide transparency
that facilitates debugging, verification, and building trust in learned
systems.

Several limitations constrain generalizability. Evaluation on Pong alone leaves
uncertain whether advantages persist in more complex environments with many
objects, stochastic dynamics, or partial observability. The 7-step imagination
horizon is a short planning window. Also, reliance on JAXAtari for ground-truth
object extraction sidesteps the challenge of learning object representations
from pixels. Future work should expand to complex Atari games (Breakout,
Seaquest, Space Invaders) that require graph neural networks or attention
mechanisms for variable numbers of objects. Investigating unsupervised object
discovery methods (for example slot attention \cite{locatello2020object}), uncertainty-aware prediction,
hierarchical \worldmodel{}s, and learned reward predictors would address key
limitations. Testing robustness to dynamic perturbations (physics parameters),
novel object configurations, and transfer learning across related environments
would reveal compositional understanding and reusable knowledge.

This thesis shows that integrating object-centric representations with
model-based reinforcement learning offers a compelling path toward more
accurate, sample-efficient, and interpretable learning systems. When task
structure is available or extractable, leveraging that structure through
appropriate inductive biases yields substantial benefits. The challenge lies in
expanding these benefits beyond simple, fully observable domains to complex,
partially observable, stochastic environments that are characteristic of
real-world applications. This requires advances in unsupervised object
discovery, long-horizon prediction, and robust generalization. By combining the
sample efficiency of model-based methods with the interpretability and
inductive bias of object-centric representations, this approach addresses two
critical challenges in modern reinforcement learning. It contributes toward
making RL systems more practical, trustworthy, and applicable to real-world
problems where both efficiency and interpretability are essential.

\printbibliography[title={References}]

\end{document}